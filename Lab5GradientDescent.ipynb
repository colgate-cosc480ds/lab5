{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 Gradient Descent\n",
    "\n",
    "In this part of the lab we will use gradient descent to fit a simple linear regression model.  You should browse the code in `simple_linear_regression.py` and `gradientdescent.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# notebook magic to display plots\n",
    "%matplotlib inline\n",
    "# notebook magic to auto reload imported modules when changes are made to them \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import simple_linear_regression as slr\n",
    "import gradientdescent as gd\n",
    "import lab5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# check that DSFS code has been cloned\n",
    "assert os.path.exists('/vagrant/data-science-from-scratch/code/')  \n",
    "sys.path.append('/vagrant/data-science-from-scratch/code/')   # add code from textbook to path\n",
    "import working_with_data as wwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "For this part of the lab we will look at data about sales (in thousands of units) for a particular product as a function of advertising budgets (in thousands of dollars) for TV, radio, and newspaper media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's load the data\n",
    "data = lab5.load_ad_data()\n",
    "print 'Dataset columns:', ','.join(data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tv = wwd.pluck('TV', data)\n",
    "sales = wwd.pluck('Sales', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the relationship between the TV ad budget and sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(tv, sales)\n",
    "plt.xlabel('Budget on TV ads')\n",
    "plt.ylabel('Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the cost function.  Examine lab5.py to make sense of the following functions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "J = slr.make_cost_function(tv, sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slr.make_contour_plot(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function appears to be minimized right around $\\beta_0 = 7.03$ and $\\beta_1 = 0.475$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "J([7.03, 0.0475])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your own words, what is J(beta0, beta1) computing?  (You may want to look at the `lab5` code as well as your notes from reading, class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**:  *todo: replace this with your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the gradient\n",
    "\n",
    "Let's calculate the gradient at the point $\\beta_0 = 0$ and $\\beta_1 = 0$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradient_fn = slr.make_gradient(tv, sales)\n",
    "gradient_fn([0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the answer tell you?  To be more specific, given the current settings of $\\beta_0, \\beta_1$, if I want to *reduce* my sum of squared error...\n",
    "\n",
    "- should I increase or decrease $\\beta_0$? \n",
    "- should I increase or decrease $\\beta_1$?  \n",
    "- which one, $\\beta_0$ or $\\beta_1$ will have a larger effect on lowering the error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**:  *todo: replace this with your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Now you will run gradient descent to find the $\\beta_0$ and $\\beta_1$ that minimizes sum of squared error on this particular dataset.\n",
    "\n",
    "To run gradient descent, you must choose the step size.  Welcome to the dark art of numerical optimization!  If you are unsure of the role that step size plays in GD, take some time now to review the code, the book, class notes, etc.\n",
    "\n",
    "This particular implementation of GD lets you specify more than step size and it will try them all and pick whichever is best in each iteration.\n",
    "\n",
    "Generally speaking, GD should take large steps at the beginning and then make progressively smaller steps.  Thus, the *largest* step size you should consider is one that is appropriate for the *first* step and then you should include some smaller step sizes too.\n",
    "\n",
    "The following code invokes GD with only a single step size of 1 and for only one iteration.  Hint: a step size of 1 is *way* too big for this problem.  Run it and see what value of `beta0` and `beta1` it returns.  Does it look familiar?  I.e., have you seen those numbers before?\n",
    "\n",
    "**TODO** Make the following modifications to GD:\n",
    "\n",
    "- Choose an appropriate set of step sizes\n",
    "- Increase the maximum number of iterations\n",
    "- Modify `minimize_batch` so that it records information while it runs (you can store the info in the `stats` dictionary that is passed in as an argument).  In particular, in each iteration, record the value of the target function.  You may wish to record other statistics too.\n",
    "\n",
    "Your goal is to use GD to find the best fit linear regression model (or pretty close to it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gd_stats = {}    \n",
    "step_sizes = [1.0]   # todo: initialize this\n",
    "theta = gd.minimize_batch(target_fn=J, \n",
    "                          gradient_fn=gradient_fn, \n",
    "                          theta_0=(0,0), \n",
    "                          max_iterations=1, \n",
    "                          step_sizes=step_sizes, \n",
    "                          stats=gd_stats)\n",
    "beta0, beta1 = theta\n",
    "print \"Gradient descent returns beta0 =\", beta0, \"and beta1 =\", beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Produce a plot in which iteration number is on the x-axis and the y-axis is the value of the target function (in this case the cost function J) at that iteration.  Your plot can *skip* some of the earliest iterations which have super high cost or alternatively you can limit the y-axis so that it is at most around 6000.  Your plot should have clearly labeled axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# todo: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent\n",
    "\n",
    "Now let's try stochastic gradient descent.  As you can see from the function call below, the target function and gradient function for SGD are different from those used in GD.\n",
    "\n",
    "In your own words, what does the specified gradient function, `slr.gradient_for_point` for SGD compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**:  *todo: replace this with your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Make the following modifications to GD:\n",
    "\n",
    "- Modify `minimize_stochastic` so that it records information while it runs (you can store the info in the `stats` dictionary).  In particular, in each iteration, record the *current* value of the target function as well as the *minimum* value seen so far.  You may wish to record other statistics too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd_stats={}\n",
    "step_size = 1./10**5  # suggested step size\n",
    "theta = gd.minimize_stochastic(target_fn=slr.cost_for_point, \n",
    "                               gradient_fn=slr.gradient_for_point, \n",
    "                               x=tv, \n",
    "                               y=sales, \n",
    "                               theta_0=(0,0), \n",
    "                               alpha_0=step_size, \n",
    "                               max_iterations=100000,  # suggested value\n",
    "                               stats=sgd_stats)\n",
    "beta0, beta1 = theta\n",
    "print \"Gradient descent returns beta0 =\", beta0, \"and beta1 =\", beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Produce a plot in which iteration number is on the x-axis and the y-axis is the value of the target function (in this case the cost function J) at that iteration.  In addition, plot the minimum value seen so far at each iteration.  Follow the same guidelines as with the previous plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# todo: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly explain any differences you observe between this figure and the similar figure you created for GD.  In particular,\n",
    "\n",
    "- Which one converges faster?  \n",
    "- Why is it important that SGD algorithm tracks the minimum value of the target function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**:  *todo: replace this with your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use gradient descent to fit a different model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use gradient descent to fit a simple linear regression model predicting sales as a function of the radio advertisement budget.\n",
    "\n",
    "**TODO** Write your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# todo: replace this with your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data as well as the best fit line that GD found.\n",
    "\n",
    "**TODO** Write your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# todo: replace this with your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge problem\n",
    "\n",
    "Modify the `gradient_descent` code to support multiple linear regression and try it out on all three predictor variables in this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
